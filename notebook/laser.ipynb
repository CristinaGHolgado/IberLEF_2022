{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "merging users: 100%|██████████| 314/314 [00:00<00:00, 469.16it/s]\n",
      "merging users: 100%|██████████| 101/101 [00:00<00:00, 1336.40it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('../data/training.xls')\n",
    "test_df = pd.read_csv('../data/development.csv')\n",
    "\n",
    "def aggregate_users(df):\n",
    "    columns_to_group_by_user = ['label', 'gender', 'profession', 'ideology_binary', 'ideology_multiclass']\n",
    "\n",
    "    group = df.groupby(by = columns_to_group_by_user, dropna = False, observed = True, sort = False)\n",
    "\n",
    "    # Custom df per user\n",
    "    df_users = group[columns_to_group_by_user].agg(func = ['count'], as_index = False, observed = True).index.to_frame (index = False)\n",
    "\n",
    "    merged_fields = []\n",
    "\n",
    "    pbar = tqdm(df_users.iterrows(), total = df_users.shape[0], desc = \"merging users\")\n",
    "\n",
    "    for index, row in pbar:\n",
    "        df_user = df[(df['label'] == row['label'])]\n",
    "        merged_fields.append({**row, **{field: ' [SEP] '.join (df_user[field].fillna ('')) for field in ['tweet']}})\n",
    "\n",
    "    df = pd.DataFrame (merged_fields)\n",
    "    return df\n",
    "\n",
    "train_df = aggregate_users(train_df)\n",
    "test_df = aggregate_users(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting laserembeddings\n",
      "  Downloading laserembeddings-1.1.2-py3-none-any.whl (13 kB)\n",
      "Collecting subword-nmt<0.4.0,>=0.3.6\n",
      "  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
      "Collecting transliterate==1.10.2\n",
      "  Downloading transliterate-1.10.2-py2.py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.15.4 in /home/amansinha/venv/global_env/lib/python3.8/site-packages (from laserembeddings) (1.20.3)\n",
      "Collecting sacremoses==0.0.35\n",
      "  Downloading sacremoses-0.0.35.tar.gz (859 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m859.8/859.8 KB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch<2.0.0,>=1.0.1.post2 in /home/amansinha/venv/global_env/lib/python3.8/site-packages (from laserembeddings) (1.9.0)\n",
      "Requirement already satisfied: six in /home/amansinha/venv/global_env/lib/python3.8/site-packages (from sacremoses==0.0.35->laserembeddings) (1.15.0)\n",
      "Requirement already satisfied: click in /home/amansinha/venv/global_env/lib/python3.8/site-packages (from sacremoses==0.0.35->laserembeddings) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/amansinha/venv/global_env/lib/python3.8/site-packages (from sacremoses==0.0.35->laserembeddings) (1.0.0)\n",
      "Requirement already satisfied: tqdm in /home/amansinha/venv/global_env/lib/python3.8/site-packages (from sacremoses==0.0.35->laserembeddings) (4.55.2)\n",
      "Collecting mock\n",
      "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/amansinha/venv/global_env/lib/python3.8/site-packages (from torch<2.0.0,>=1.0.1.post2->laserembeddings) (3.7.4.3)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.35-py3-none-any.whl size=883989 sha256=ff7bbb80df7c6b7951b0108773870990b6addf81837baae7fe59254ef58853df\n",
      "  Stored in directory: /home/amansinha/.cache/pip/wheels/c4/df/30/3d6c623db99d503dcdbae1f686953b7c1a0754d8a658dc0845\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: transliterate, sacremoses, mock, subword-nmt, laserembeddings\n",
      "  Attempting uninstall: sacremoses\n",
      "    Found existing installation: sacremoses 0.0.43\n",
      "    Uninstalling sacremoses-0.0.43:\n",
      "      Successfully uninstalled sacremoses-0.0.43\n",
      "Successfully installed laserembeddings-1.1.2 mock-4.0.3 sacremoses-0.0.35 subword-nmt-0.3.8 transliterate-1.10.2\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/home/amansinha/venv/global_env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install laserembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading models into /home/amansinha/venv/global_env/lib/python3.8/site-packages/laserembeddings/data\n",
      "\n",
      "✅   Downloaded https://dl.fbaipublicfiles.com/laser/models/93langs.fcodes    \n",
      "✅   Downloaded https://dl.fbaipublicfiles.com/laser/models/93langs.fvocab    \n",
      "✅   Downloaded https://dl.fbaipublicfiles.com/laser/models/bilstm.93langs.2018-12-26.pt    \n",
      "\n",
      "✨ You're all set!\n"
     ]
    }
   ],
   "source": [
    "!python -m laserembeddings download-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00156426, 0.01067378, 0.00388247, ..., 0.021797  , 0.0062856 ,\n",
       "        0.01434554],\n",
       "       [0.01456157, 0.00154333, 0.00090701, ..., 0.01484861, 0.00712931,\n",
       "        0.0476219 ]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from laserembeddings import Laser\n",
    "\n",
    "laser = Laser()\n",
    "\n",
    "# if all sentences are in the same language:\n",
    "\n",
    "embeddings = laser.embed_sentences(\n",
    "    ['let your neural network be polyglot',\n",
    "     'use multilingual embeddings!'],\n",
    "    lang='es')\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314/314 [20:47<00:00,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(314, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "for sen in tqdm(train_df.tweet):\n",
    "        \n",
    "    embeds = laser.embed_sentences(sen.split('[SEP]')\n",
    "                            ,lang='es')\n",
    "    \n",
    "    \n",
    "    x_train.append(np.mean(embeds,axis=0))\n",
    "x_train = np.asarray(x_train)\n",
    "print(x_train.shape)\n",
    "# save the embeddings, because it will take time to get them everytime\n",
    "np.save('xtrain_raw_laser.npy',x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [03:50<00:00,  2.28s/it]\n"
     ]
    }
   ],
   "source": [
    "def return_wvecs(train_df):\n",
    "    x_train = []\n",
    "    for sen in tqdm(train_df.tweet):\n",
    "        \n",
    "        embeds = laser.embed_sentences(sen.split('[SEP]')\n",
    "                            ,lang='es')\n",
    "        x_train.append(np.mean(embeds,axis=0))\n",
    "        \n",
    "    x_train = np.asarray(x_train)\n",
    "    return x_train\n",
    "\n",
    "#xtrain = return_wvecs(train_df)\n",
    "xtrain = np.load('xtrain_raw_laser.npy')\n",
    "xtest = return_wvecs(test_df)\n",
    "np.save('xtest_raw_laser.npy',x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = train_df.gender\n",
    "ytest = test_df.gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38 10]\n",
      " [17 36]]\n",
      "0.7323062403327711\n"
     ]
    }
   ],
   "source": [
    "m = SVC(class_weight='balanced')\n",
    "\n",
    "m.fit(xtrain, ytrain)\n",
    "predictions = m.predict(xtest)\n",
    "cm =confusion_matrix(ytest, predictions)\n",
    "cr = classification_report(ytest, predictions, zero_division = 0, output_dict=True)\n",
    "print(cm)\n",
    "print(cr['weighted avg']['f1-score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| laser | 0.7323 | 0.8784 | 0.8812 | 0.7371 |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f1s = []\n",
    "for label in ['gender', \t'profession' ,\t'ideology_binary', \t'ideology_multiclass']:\n",
    "    ytrain = train_df[label]\n",
    "    ytest = test_df[label]\n",
    "    m = SVC(class_weight='balanced')\n",
    "\n",
    "    m.fit(xtrain, ytrain)\n",
    "    predictions = m.predict(xtest)\n",
    "    cm =confusion_matrix(ytest, predictions)\n",
    "    \n",
    "    score = f1_score(ytest, predictions, average='micro')\n",
    "    cr = classification_report(ytest, predictions, output_dict=True)\n",
    "    f1s.append(cr['weighted avg']['f1-score'])\n",
    "\n",
    "print(f'| laser | {f1s[0]:.4f} | {f1s[1]:.4f} | {f1s[2]:.4f} | {f1s[3]:.4f} |')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|model| f1-gender | f1-profession | f1-ib | f1-im|\n",
    "|---|---|---|---|---|\n",
    "| laser | 0.7323 | 0.8784 | 0.8812 | 0.7371 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "References : \n",
    "    \n",
    "- https://github.com/yannvgn/laserembeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
