{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_dim=100, output_dim=1, n_layers=1):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.device = torch.device('cpu')\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(self.n_layers,x.shape[0],self.hidden_dim)\n",
    "        #print(x.shape[0])\n",
    "        hidden,cell = torch.zeros(self.n_layers, x.size(0),self.hidden_dim),torch.zeros(self.n_layers,x. size(0),self.hidden_dim)\n",
    "            \n",
    "        # Initialization fo hidden and cell states\n",
    "        torch.nn.init.xavier_normal_(hidden)\n",
    "        torch.nn.init.xavier_normal_(cell)\n",
    "        \n",
    "        out, (hidden, cell) = self.lstm(x, (hidden,cell))\n",
    "        #print(\"out shape:\", out[:,-1,:].shape)\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "merging users: 100%|██████████| 314/314 [00:00<00:00, 478.81it/s]\n",
      "merging users: 100%|██████████| 101/101 [00:00<00:00, 1305.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('../data/training.xls')\n",
    "test_df = pd.read_csv('../data/development.csv')\n",
    "\n",
    "def aggregate_users(df):\n",
    "    columns_to_group_by_user = ['label', 'gender', 'profession', 'ideology_binary', 'ideology_multiclass']\n",
    "\n",
    "    group = df.groupby(by = columns_to_group_by_user, dropna = False, observed = True, sort = False)\n",
    "\n",
    "    # Custom df per user\n",
    "    df_users = group[columns_to_group_by_user].agg(func = ['count'], as_index = False, observed = True).index.to_frame (index = False)\n",
    "\n",
    "    merged_fields = []\n",
    "\n",
    "    pbar = tqdm(df_users.iterrows(), total = df_users.shape[0], desc = \"merging users\")\n",
    "\n",
    "    for index, row in pbar:\n",
    "        df_user = df[(df['label'] == row['label'])]\n",
    "        merged_fields.append({**row, **{field: ' [SEP] '.join (df_user[field].fillna ('')) for field in ['tweet']}})\n",
    "\n",
    "    df = pd.DataFrame (merged_fields)\n",
    "    return df\n",
    "\n",
    "train_df = aggregate_users(train_df)\n",
    "test_df = aggregate_users(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314/314 [00:02<00:00, 116.77it/s]\n",
      "100%|██████████| 101/101 [00:01<00:00, 100.37it/s]\n"
     ]
    }
   ],
   "source": [
    "def return_wvecs(train_df):\n",
    "    x_train = []\n",
    "    for sen in tqdm(train_df.tweet):\n",
    "        \n",
    "        sens = []\n",
    "        for ss in sen.split('[SEP]'):\n",
    "            sens.extend(ss.split(' '))\n",
    "        if len(sens) < 6000:\n",
    "            diff= 6000 - len(sens)\n",
    "            sens.extend(['[PAD]']*diff)\n",
    "        else:\n",
    "            sens = sens[:6000]\n",
    "        #print(len(sens))\n",
    "        wvs = []\n",
    "        for w in sens:\n",
    "            #print(w)\n",
    "            try:\n",
    "                wvs.append(model.wv[w])\n",
    "            except:\n",
    "                #print('hi', w)\n",
    "                wvs.append(np.zeros(100))\n",
    "        \n",
    "        wvs = np.asarray(wvs)\n",
    "        #print(wvs.shape)\n",
    "        x_train.append(wvs)#np.mean(wvs, axis=0))\n",
    "        \n",
    "    x_train = np.asarray(x_train)\n",
    "    return x_train\n",
    "\n",
    "xtrain = return_wvecs(train_df)\n",
    "xtest = return_wvecs(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(314, 6000, 100)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = train_df.gender\n",
    "ytest = test_df.gender\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "le.fit(ytrain)\n",
    "ytrain = le.transform(ytrain)\n",
    "ytest = le.transform(ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class taskdata(Dataset):\n",
    "    def __init__(self, x_train, y_train):\n",
    "        self.xtrain = x_train\n",
    "        self.ytrain = y_train\n",
    "        \n",
    "    def __len__(self): return len(self.xtrain)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.xtrain[idx, : ,:], self.ytrain[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataloader= DataLoader(taskdata(xtrain,ytrain), 32, shuffle=True)\n",
    "testdataloader= DataLoader(taskdata(xtest, ytest), 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, params, num=None):\n",
    "    \n",
    "    #input_dim = inpdim\n",
    "    device = torch.device('cpu')\n",
    "    #print(f\"working with {inpdim} features\")#nextitem(iter(trainloader))[0].shape[2]\n",
    "    output_dim = 1\n",
    "    n_layers = 2\n",
    "    batch_size= 32\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    \n",
    "    #if num:\n",
    "        #writer = SummaryWriter(f'runs/gru_experiment_{num}')\n",
    "        \n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    epoch_times  = []\n",
    "    \n",
    "    p =0\n",
    "    \n",
    "    for epoch in range(params['epochs']):\n",
    "        \n",
    "        start = time.time()\n",
    "        avg_loss = 0\n",
    "        counter = 0\n",
    "        \n",
    "        for x, label in tqdm(trainloader, desc='Training'):\n",
    "            #print(x.shape)\n",
    "            counter +=1\n",
    "            p +=1\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = model(x.to(device).float())\n",
    "            \n",
    "            out = torch.squeeze(out, dim=0)\n",
    "            loss = criterion(out, label.to(device))\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "            if counter % 200 == 0:\n",
    "                print(f\"Epoch : {epoch}, Step: {counter}/{len(trainloader)} ==> Avg Loss for epoch: {avg_loss/counter}\")\n",
    "                #if num: writer.add_scalar('training_loss', avg_loss/counter, p)\n",
    "        print(f\"epoch {epoch+1}, loss {avg_loss}\")        \n",
    "                \n",
    "        current_time = time.time()\n",
    "        #print(f\"Total time elapsed: {current_time-start} seconds\")\n",
    "        epoch_times.append(current_time- start)\n",
    "    \n",
    "    #print(f\"Total Training Time: {sum(epoch_times)} seconds\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'epochs': 10, \n",
    "         'lr' : 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:34<00:00,  3.42s/it]\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 6.815458416938782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:34<00:00,  3.49s/it]\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 6.66422826051712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:35<00:00,  3.58s/it]\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss 6.604955196380615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:36<00:00,  3.65s/it]\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss 6.5525548458099365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:38<00:00,  3.89s/it]\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, loss 6.4270671010017395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:39<00:00,  3.93s/it]\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, loss 6.256490409374237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:41<00:00,  4.12s/it]\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, loss 6.157841324806213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:43<00:00,  4.34s/it]\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, loss 5.892782688140869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:45<00:00,  4.51s/it]\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, loss 5.7959787249565125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:47<00:00,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss 5.651024222373962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = LSTMNet(input_size=100, output_dim=2)\n",
    "net = train(net, traindataloader, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, testdataloader, params):\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for x,y in tqdm(testdataloader, desc = 'Evaluating'):\n",
    "        x = x.type(torch.DoubleTensor)\n",
    "        #print(x.dtype)\n",
    "        out = model(x.to(device).float())\n",
    "        out = torch.squeeze(out, dim=0)\n",
    "        #print(out.argmax())\n",
    "        outputs.append(out.argmax().cpu().detach().numpy())\n",
    "        targets.extend(y.numpy())\n",
    "        #break\n",
    "        \n",
    "    return np.asarray(outputs), np.asarray(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 101/101 [00:43<00:00,  2.31it/s]\n"
     ]
    }
   ],
   "source": [
    "o,t = evaluate(net, testdataloader, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        48\n",
      "           1       0.52      1.00      0.69        53\n",
      "\n",
      "    accuracy                           0.52       101\n",
      "   macro avg       0.26      0.50      0.34       101\n",
      "weighted avg       0.28      0.52      0.36       101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "score = f1_score(t, o, average='micro')\n",
    "cr = classification_report(t, o)\n",
    "\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reference :  \n",
    "    \n",
    "-https://stackoverflow.com/questions/61632584/understanding-input-shape-to-pytorch-lstm\n",
    "    \n",
    "-https://towardsdatascience.com/text-classification-with-pytorch-7111dae111a6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
